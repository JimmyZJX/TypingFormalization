\section{Introduction}

Modern functional programming languages, such as Haskell or OCaml,
use sophisticated forms of type inference. The type systems of these languages are
descendants of
Hindley-Milner~\cite{hindley1969principal,milner1978theory,damas1982principal}, 
which was revolutionary at the
time in allowing type-inference to proceed without any type
annotation. The traditional Hindley-Milner type system supports
top-level \emph{implicit (parametric) polymorphism}~\cite{reynolds1983types}. With implicit
polymorphism, type arguments of polymorphic functions are
automatically instantiated. Thus implicit polymorphism and the absence of
type annotations mean that the Hindley-Milner type system 
strikes a great balance between expressive power and usability. 

As functional languages evolved the need for more expressive
power has motivated language designers to look beyond Hindley-Milner.
In particular one popular direction is to allow 
\emph{higher-ranked polymorphism} where polymorphic types can
occur anywhere in a type signature.  
An important challenge is that full type inference for higher-ranked
polymorphism is known to be undecidable~\cite{wells1999typability}. Therefore some type
annotations are necessary to guide type inference. In response to this 
challenge several decidable type systems requiring some annotations 
have been proposed~\cite{,dunfield2013complete,jones2007practical,Serrano2018,le2003ml,leijen2008hmf,vytiniotis2008fph}.
Two closely related type systems that 
support \emph{predicative} higher-ranked type inference were proposed 
by Peyton Jones et al.~\cite{jones2007practical} and Dunfield and
Krishnaswami~\cite{dunfield2013complete} (henceforth denoted as DK). 
These type systems are
popular among language designers and their ideas have been adopted by
several modern functional languages, including Haskell, PureScript~\cite{PureScript} and
Unison~\cite{Unison} among others.
In those type systems
type annotations are required for polymorphic arguments of functions,
but other type annotations can be omitted. A canonical example (here written in Haskell) is:
\begin{verbatim}
    hpoly = \(f :: forall a. a -> a) -> (f 1, f 'c')
\end{verbatim}
The function \verb|hpoly| cannot be
type-checked in the Hindley-Milner type system. The type of \verb|hpoly| is the rank-2 type:
\verb|(forall a. a -> a) -> (Int, Char)|. Notably (and unlike
Hindley-Milner) the lambda argument \verb|f| requires a
\emph{polymorphic} type annotation.
This annotation is needed because the single universal quantifier
does not appear at the top-level. Instead it is used to quantify a
type variable \verb|a| used in the first argument of the
function. 
Despite these additional annotations, Peyton Jones et al. and DK's
type inference algorithms preserve many of the desirable properties 
of Hindley-Milner. For example the applications of \verb|f| implicitly 
instantiate the polymorphic type arguments of \verb|f|.

Although type inference is important in practice and receives a lot of
attention in
academic research, there is little work on mechanically formalizing
such advanced forms of type inference in theorem provers.
The remarkable exception is work done on the formalization of 
certain parts of Hindley-Milner type inference~\cite{naraschewski1999type,
dubois2000proving,dubois1999certification,urban2008nominal,
garrigue2015certified}. However
there is still no formalization of the higher-ranked type systems
that are employed by modern languages like Haskell.
This is at
odds with the current trend of mechanical formalizations in
programming language research. In particular both the POPLMark
challenge~\cite{aydemir2005mechanized} and
CompCert ~\cite{leroy2012compcert} have significantly promoted
the use of theorem provers to model various aspects of programming
languages. Today papers in various programming language venues routinely
use theorem provers to mechanically formalize: \emph{dynamic and
  static semantics} and their correctness properties~\cite{aydemir2008engineering},
\emph{compiler correctness}~\cite{leroy2012compcert}, \emph{correctness of
  optimizations}~\cite{Bertot04}, \emph{program analysis}~\cite{Chang2006}
or proofs involving \emph{logical relations}~\cite{abel2018}. The
main argument for mechanical formalizations is a simple one. Proofs
for programming languages tend to be \emph{long}, \emph{tedious} and
\emph{error-prone}. In such proofs it is very easy to make mistakes
that may invalidate the whole development. Furthermore, readers and
reviewers often do not have time to look at the proofs carefully to
check their correctness. Therefore errors can go unnoticed for a
long time.  Mechanical formalizations provide, in principle, a natural
solution for these problems. Theorem provers can automatically check and
validate the proofs, which removes the burden of checking from both
the person doing the proofs as well as readers or reviewers.

This paper presents the first fully mechanized formalization of the
metatheory for higher-ranked polymorphic type inference.
The system
that we formalize is the bidirectional type system by \citet{dunfield2013complete}.
We chose DK's type system because it is
quite elegant, well-documented and it comes with detailed manually
written proofs. Furthermore the system is adopted in practice by a few
real implementations of functional languages, including PureScript and
Unison. The DK type system has two variants: a declarative
and an algorithmic one. The two variants have been
\emph{manually} proved to be \emph{sound}, \emph{complete} and
\emph{decidable}.
We present a mechanical formalization in the Abella theorem prover~\cite{AbellaDesc} for
DK's declarative type system using a different algorithm. While our
initial goal was to formalize both DK's declarative and algorithmic
versions, we faced technical challenges with the latter, prompting us to find
an alternative formulation.

The first challenge that we faced were missing details as well as
a few incorrect proofs and lemmas in DK's formalization. While DK's
original formalization comes with very well written manual proofs,
there are still several details missing. These complicate the task of
writing a mechanically verified proof. Moreover some proofs and
lemmas are wrong and, in some cases, it is not clear to us how to fix them.
%%\footnote{
%%Perhaps, with some additional thought, a work-around can be found, but, as our algorithm
%%differs significantly from DK's, it would not further our mechanization goal and so we did not pursue this.}
Despite the problems in DK's manual formalization,
we believe that these problems do not
invalidate their work and that their results are still true. In fact we have nothing but praise for their detailed
and clearly written metatheory and proofs, which provided invaluable
help to our own work.
We expect that for most non-trivial manual
proofs similar problems exist, so this should not be understood as a sign of sloppiness
on their part. Instead it should be an indicator that reinforces the arguments
for mechanical formalizations: manual formalizations are error-prone due to the multiple
tedious details involved in them.
There are several other examples of manual formalizations that were found to have
similar problems. For example, Klein et al.~\cite{KleinRunYourResearch}
mechanized formalizations
in Redex for nine ICFP 2009 papers and all were found to have mistakes.

Another challenge was variable binding. Type inference algorithms
typically do not rely simply on local environments but instead
propagate information across judgments. While local environments are
well-studied in mechanical formalizations, there is little work on how
to deal with the complex forms of binding employed by type inference algorithms
in theorem provers. To
keep track of variable scoping, DK's algorithmic version employs input
and output contexts to track information that is discovered through
type inference. However modeling output contexts in a theorem prover
is non-trivial.

Due to those two challenges, our work takes a different approach by refining and
extending the idea of \emph{worklist judgments}~\cite{itp2018},
proposed recently to mechanically formalize an algorithm for
\emph{polymorphic subtyping}~\cite{odersky1996putting}. A key innovation in our work is how
to adapt the idea of worklist judgments to
\emph{inference judgments}, which are not needed for polymorphic
subtyping, but are necessary for type-inference.  The idea is to use a \emph{continuation
passing style} to enable the transfer of inferred information across
judgments. A further refinement to the idea of worklist judgments is
the \emph{unification between ordered
  contexts~\cite{gundry2010type,dunfield2013complete} and worklists}.  This
enables precise scope tracking of free variables in
judgments. Furthermore it avoids the duplication of context
information across judgments in worklists that occurs in other
techniques~\cite{Reed2009,Abel2011higher}.
Despite the use of a different algorithm we prove the
same results as DK, although with significantly different proofs and
proof techniques. The calculus and its metatheory
have been fully formalized in the Abella theorem prover~\cite{AbellaDesc}.

In summary, the contributions of this paper are:

\begin{itemize}

\item {\bf A fully mechanized formalization of type inference with
  higher-ranked types:} Our work presents the first fully mechanized formalization
  for type inference of higher ranked types. The formalization is done in the
  Abella theorem prover~\cite{AbellaDesc} and it is available
  online at \url{http://TODO}.\bruno{fill url please!}

\item {\bf A new algorithm for DK's type system:} Our work proposes a novel algorithm that implements
  DK's declarative bidirectional type system. We prove
  \emph{soundness}, \emph{completeness} and
  \emph{decidability}. 

\item {\bf Worklists with inference judgments:} One technical contribution is the
  support for inference judgments using worklists. The idea is to
  use a continuation passing style to enable the transfer of inferred information across
  judgments. 

\item {\bf Unification of worklists and contexts:} Another technical contribution is the unification
  between ordered contexts and worklists. This enables precise scope tracking
  of variables in judgments, and avoids the duplication of context information across
  judgments in worklists.

\begin{comment}
\jimmy{Notes @20190211 4 points of novalty:\\
1) Dealing with inference judgments and CPS-style chains\\
2) The form of the judgment itself with a single shared context\\
3) The way we deal with scope (which may follow from 2)\\
4) Immediate substitution (judgment list)
}
\end{comment}

\end{itemize}
