\section{Overview}

\bruno{I think some text/ideas-for-text can be borrowed from ITP.}

\subsection{DK's Declarative System}
% Subtyping and Typing. \jimmy{Can you prepare the figures. Subtyping is already in ITP.}

\begin{figure}[t]
\[
\begin{array}{l@{\qquad}lcl}
\text{Type variables}\qquad&a, b\\[3mm]
\text{Types}\qquad&A, B, C &::=&\quad 1 \mid a \mid \forall a. A \mid A\to B\\
\text{Monotypes}\qquad&\tau,\sigma &::=&\quad 1 \mid a \mid \tau\to \sigma\\
\text{Contexts}\qquad&\Psi &::=&\quad \nil \mid \Psi, a\\
%\text{Judgments}\qquad&\exps &::=&\quad \nil \mid A \le B : \exps
\end{array}
\]
\caption{Syntax of Declarative System}\label{fig:decl:syntax}
\end{figure}

The syntax of this declarative system is shown in Figure~\ref{fig:decl:syntax}.
A declarative type could be a unit type 1, a type variable $a$,
a universal quantification $\forall a. A$ or a function type $A \to B$.
Nested universal quantifiers are allowed for types,
but monotypes do not have any universal quantifier.
Context $\Psi$ is just a collection of type variable declarations.

\begin{figure}[t]
%\centering \framebox{$\Psi \vdash A$}
%\begin{gather*}
%\inferrule*[right=$\mathtt{wf_d unit}$]
%    {~}{\Psi\vdash 1}
%\qquad
%\inferrule*[right=$\mathtt{wf_d var}$]
%    {a\in\Psi}{\Psi\vdash a}
%\qquad
%\inferrule*[right=$\mathtt{wf_d{\to}}$]
%    {\Psi\vdash A\quad \Psi\vdash B}
%    {\Psi\vdash A\to B}
%\qquad
%\inferrule*[right=$\mathtt{wf_d\forall}$]
%    {\Psi, a\vdash A}
%    {\Psi\vdash \forall a. A}
%\end{gather*}

\centering \framebox{$\Psi \vdash A \le B$}
\begin{gather*}
\inferrule*[right=$\mathtt{{\le}Var}$]
    {a\in\Psi}{\Psi\vdash a\le a}
\qquad
\inferrule*[right=$\mathtt{{\le}Unit}$]
    {~}{\Psi \vdash 1 \le 1}
\qquad
\inferrule*[right=$\mathtt{{\le}{\to}}$]
    {\Psi \vdash B_1 \le A_1 \quad \Psi \vdash A_2 \le B_2}
    {\Psi\vdash A_1\to A_2 \le B_1\to B_2}
\\
\inferrule*[right=$\mathtt{{\le}\forall L}$]
    {\Psi\vdash \tau \quad \Psi\vdash [\tau/a] A \le B}
    {\Psi\vdash \forall a. A \le B}
\qquad
\inferrule*[right=$\mathtt{{\le}\forall R}$]
    {\Psi, a\vdash A\le B}
    {\Psi\vdash A \le \forall a. B}
\end{gather*}
\caption{%Well-formedness of Declarative Types and 
Declarative Subtyping}\label{fig:decl:sub}
\end{figure}

\begin{figure}[t]
\centering \framebox{$\Psi \vdash A \Lto B$} $e$ checks against input type $A$.\\
\centering \framebox{$\Psi \vdash A \To B$} $e$ synthesizes output type $A$.\\
\centering \framebox{$\Psi \vdash \appInf{A}{e}{C}$} Applying a function of type $A$ to $e$ synthesizes type $C$.
\begin{gather*}
\inferrule*[right=$\mathtt{DeclVar}$]
    {(x:A)\in\Psi}{\Psi\vdash x\To A}
\qquad
\inferrule*[right=$\mathtt{DeclSub}$]
%e \neq \lam e' \quad B \neq \all B' \quad 
    {\Psi\vdash e\To A \quad \Psi\vdash A\le B}
    {\Psi \vdash e\Lto B}
\\
\inferrule*[right=$\mathtt{DeclAnno}$]
    {\Psi \vdash A \quad \Psi\vdash e\Lto A}
    {\Psi\vdash (e:A)\To A}
\qquad
\inferrule*[right=$\mathtt{Decl1I}$]
    {~}{\Psi\vdash () \Lto 1}
\qquad
\inferrule*[right=$\mathtt{Decl1I\To}$]
    {~}{\Psi\vdash () \To 1}
\\
\inferrule*[right=$\mathtt{Decl\forall I}$]
    {\Psi,a \vdash e \Lto A}
    {\Psi\vdash e\Lto \all A}
\qquad
\inferrule*[right=$\mathtt{Decl\forall App}$]
    {\Psi \vdash \tau \quad \Psi\vdash \appInf{[\tau/a]A}{e}{C} }
    {\Psi\vdash \appInf{\all A}{e}{C}}
\\
\inferrule*[right=$\mathtt{Decl\to I}$]
    {\Psi,x:A \vdash e\Lto B}
    {\Psi\vdash \lam e \Lto A \to B}
\qquad
\inferrule*[right=$\mathtt{Decl\to I\To}$]
    {\Psi\vdash \sigma\to\tau \quad \Psi,x:\sigma \vdash e\Lto \tau}
    {\Psi\vdash \lam e \To \sigma\to\tau}
\\
\inferrule*[right=$\mathtt{Decl\to E}$]
    {\Psi\vdash e_1\To A \quad \Psi\vdash \appInf{A}{e_2}{C}}
    {\Psi\vdash e_1~e_2 \To C}
\qquad
\inferrule*[right=$\mathtt{Decl\to App}$]
    {\Psi\vdash e \Lto A}
    {\Psi\vdash \appInf{A \to C}{e}{C}}
\end{gather*}
\caption{Declarative Typing}\label{fig:decl:typing}
\end{figure}

In Figures~\ref{fig:decl:sub} and \ref{fig:decl:typing},
we give the subtyping and typing relations for the declarative system,
which is exactly the same as DK's.
The subtyping relations originate from Odersky and La\"ufer~\cite{odersky1996putting},
and is adopted by DK as a part of their typing system.

\paragraph{Declarative Subtyping}
For implicit polymorphic type systems,
subtyping relation compares the degree of polymorphism of types.
In short, if a polymorphic type $A$ always succeed to instantiate to any instantiation of $B$,
then A is ``at least as polymorphic as'' $B$,
or we just say that $A$ is ``more polymorphic than'' $B$, written as $A \le B$.

Subtyping rules $\mathtt{{\le}Var}$, $\mathtt{{\le}Unit}$ and $\mathtt{{\le}{\to}}$
handle simple cases where no universal quantifiers get involved.
The subtyping rule for function types $\mathtt{{\le}{\to}}$ is standard,
being covariant on the return type and contravariant on the argument type.
Rule $\mathtt{{\le}\forall L}$ states that if $A$ is a subtype of $B$
under the context ($\Psi, a$), where $a$ is fresh in $A$, then $A\le\all B$.
Intuitively, if $A$ is more general than $\all B$ (where the universal quantifier
already indicates that $\all B$ is a general type),
then $A$ must instantiate to $[\tau/a]B$ for every $\tau$.

The most interesting rule is $\mathtt{{\le}\forall R}$.
If any instantiation of $\all A$, $[\tau/a]A$, is a subtype of $B$,
then $\all A \le B$.
The monotype $\tau$ we used to instantiation is \emph{guessed} for the declarative rule,
but the algorithmic system should be able to compute the guess properly.
The fact that $\tau$ is a monotype rules out the possibility of
polymorphic (or impredicative) instantiation.
Peyton Jones et al.~\cite{} also has such restriction of
predicative instantiation in their type system.
Both systems are adopt by several practical programming languages.

\paragraph{Declarative Typing}
\jimmy{TODO decl. typing description}

\paragraph{Overlapping Rules}
\jimmy{Move to the Alg. Section}
Some of the declarative rules overlaps with each other.
Declarative subtyping rules $\mathtt{{\le}\forall L}$ and $\mathtt{{\le}\forall R}$
both match the conclusion $\Psi\vdash \all A \le \all B$.
In such case, choose $\mathtt{{\le}\forall R}$ first is always better,
since we introduce the type variable $a$ to the context earlier,
which gives more flexibility on the choice of $\tau$.

Declarative typing rule $\mathtt{DeclSub}$ overlaps with
both $\mathtt{Decl\forall I}$ and $\mathtt{Decl\to I}$.
However, we could argue that more specific rules are always the best choices,
i.e. $\mathtt{Decl\forall I}$ and $\mathtt{Decl\to I}$ should have
higher priority to $\mathtt{DeclSub}$.
For example, $\Psi\vdash \lam x \Lto \all a\to a$ succeeds if derived from
Rule $\mathtt{Decl\forall I}$, but fails when applied to $\mathtt{DeclSub}$:
$$
\inferrule*[right={$\mathtt{Decl\forall I}$}]
	{\inferrule*[Right={$\mathtt{Decl\to I}$}]
		{\Psi,a,x:a \vdash x \Lto a}
		{\Psi,a \vdash \lam x \Lto a \to a}
	}
	{\Psi\vdash \lam x \Lto \forall a \to a}
$$
$$
\inferrule*[right={$\mathtt{DeclSub}$}]
	{
		\inferrule*[right=$\mathtt{Decl\to I\To}$]
			{\Psi \vdash \blue\sigma\to \blue\tau \quad \Psi,x:\blue\sigma\vdash e \Lto \blue\tau}
			{\Psi \vdash \lam x \To \blue\sigma\to \blue\tau}\quad
		\inferrule*[Right=$\mathtt{{\le}\forall R}$]
			{\inferrule*[Right=$\mathtt{{\le}{\to}}$]
				{
					\inferrule*[Right=$?$]
						{\text{Impossible! }\blue\sigma \neq a}
						{\Psi,a \vdash a \le \blue\sigma}
					\quad \Psi,a \vdash \blue\tau \le a
				}
				{\Psi,a \vdash \blue\sigma\to \blue\tau \le a \to a}
			}
			{\Psi\vdash \blue\sigma\to \blue\tau\le \all a \to a}
	}
{\Psi\vdash \lam x \Lto \forall a \to a}
$$

Rule $\mathtt{Decl\to I}$ is also better at handling higher-order types.
When the lambda-expression to be inferred has a polymorphic input type,
such as $\all a \to a$,
$\mathtt{DeclSub}$ may not derive some judgments.
For example, $\Psi,id:\all a\to a \vdash \lam[f] f~id~(f~()) \Lto (\all a\to a) \to 1$
requires the argument of the lambda-expression to be a polymorphic type,
otherwise it could not be applied to both $id$ and $()$.
If Rule $\mathtt{DeclSub}$ was chosen for derivation,
the type of its argument is restricted by Rule $\mathtt{Decl\to I\To}$,
which is not a polymorphic type.
By contrast,
Rule $\mathtt{Decl\to I}$ keeps the polymorphic argument type $\all a\to a$,
and will successfully derive the judgment.

%-------------------------------------------------------------------------------
\subsection{DK's Algorithm}

The DK algorithm revolves around their notion of \emph{algorithmic context}.
\[
\begin{array}{l@{\qquad}lcl}
\text{Algorithmic Contexts}\qquad&\Gamma,\Delta,\Theta &::=&\quad \nil \mid
\Gamma, a \mid \Gamma, \hat{\alpha} \mid \Gamma, \hat{\alpha} = \tau \mid
\Gamma, \blacktriangleright_{\hat{\alpha}}
\end{array}
\]
In addition to the regular (universally quantified) type variables $a$, the
algorithmic context also contains \emph{existential} type variables
$\hat{\alpha}$. These are placeholders for monotypes $\tau$ that are still to
be determined by the inference algorithm. When the existential variable is
``solved'', its entry in the context is replaced by the assignment
$\hat{\alpha} = \tau$. With the notation $[\Gamma]$ an algorithmic context
$\Gamma$ denotes the substitution that replaces all its solved existential type
variables with their solution.

All algorithmic judgments thread an algorithmic context. They have the form
$\Gamma \vdash \ldots \dashv \Delta$, where $\Gamma$ is the input context and
$\Delta$ is the output context:
$\Gamma \vdash A \le B \dashv \Delta$  for subtyping, 
$\Gamma \vdash e \Leftarrow A \dashv \Delta$  for type checking, and so on. 
The output context is a functional update of the input context that records newly
introduced existentials and solutions.

Solutions are incrementally propagated by applying the algorithmic output
context of a previous task as substitution to the next task. This can be seen
in the subsumption rule:
\[
\inferrule*
  {\Gamma \vdash e \Rightarrow A \dashv \Theta \\ 
   \Theta \vdash [\Theta]A \le [\Theta]B \dashv \Delta
  }
  { \Gamma \vdash e \Leftarrow B \dashv \Delta}
\]
The inference task yields an output context $\Theta$ which is applied as a substitution
to the types $A$ and $B$ before performing the subtyping check to propagate any solutions
of existential variables that appear in $A$ and $B$.

The sequential order of entries in the algoritmic context, in combination with
the threading of contexts,  does not perfectly capture the scoping of all
existential variables. For this reason the DK algorithm uses scope markers
$\blacktriangleright_{\hat{\alpha}}$ in a few places. An example is given in the following
rule:
\[
\inferrule*
  {\Gamma,\blacktriangleright_{\hat{\alpha}}, \hat{\alpha} \vdash [\hat{\alpha}/a]A \le B \dashv \Delta,\blacktriangleright_{\hat{\alpha}},\Theta}
  {\Gamma \vdash \forall a. A \le B \dashv \Delta}
\]
To indicate that the scope of $\hat{\alpha}$ is local to the subtyping check
$[\hat{\alpha}/a]A \le B$, the marker is pushed onto its input stack and popped
from the output stack together with the subsequent part $\Theta$, which may
refer to $\hat{\alpha}$. (Remember that later entries may refer to earlier
ones, but not vice versa.) This way $\hat{\alpha}$ does not escape its scope.


At first sight, the DK algorithm would seem a good basis for mechanisation. After
all it comes with a careful description and extensive manual proofs.
Unfortunately, we ran into several obstacles that have prompted us to formulate
a different, more mechanisation-friendly algorithm.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
\paragraph{Broken Metatheory} % Talk about the lemmas that are false, show counterexample.
While going through the manual proofs of DK's algorithm, we found several problems.
The proof of Lemma 19 (Extension Equality Preservation) applies a wrong
induction hypothesis for the ${\longrightarrow}\mathtt{Uvar}$ case.
Fortunately the lemma could be proven without the well-formedness conditions
$\Gm\vdash A$ and $\Gm\vdash B$.
Similar problem happens for Lemma 14 (Subsumption), where many applications of
induction hypotheses are not correct.
For manual proofs, induction hypotheses are not automatically generated by programs,
thus are easily misused.

Moreover, Lemma 29 (Parallel Admissibility) is false.
One may find a counterexample with detailed explaination in the appendix.
Lemma 29 is an important lemma for the proof of soundness and completeness,
which are two main results of their metatheory.
Instantiation soundness (i.e. a part of subtyping soundness) and typing completeness
both directly require Lemma 29.
Lemma 54 (Typing Extension) requires Lemma 29 and is applied 13 times
in the proof of typing soundness and completeness.

%- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
\paragraph{Complex Scoping and Propagation}

Besides the problematic lemmas in the DK metatheory, there are other reasons to
look for an alternative algorithmic formulation of the type system that is more
amenable to mechanisation. Indeed, two aspects that are particularly
challenging to mechanise are the scoping of universal and existential type
variables, and the propagation of the instantiation of existential type
variables across judgments. 

DK is already quite disciplined on these accounts, in particular compared to
traditional constraint-based type-inference algorithms like Algorithm W which
features an implicit global scope for all type variables. Indeed, DK uses an
explicit and ordered context that tracks the relative scope of universal and
existential variables and it is careful to only instantiate existential
variables in a well-scoped manner.

Moreover, DK's algorithm carefully propagates instantiations by recording them
into the context and threading this context through all judgments.  means of
two This means that every judgments takes two contexts---an input and an output
context---rather than the conventional single context. The output context
records any new variable instantations; to propagate these instantiations to
the remaining judgements, their predecessor's output context---which is their
input context---is applied to them as a substitution.

While it works, DK's approach is still fairly involved and thus hard to reason
about in a mechanised setting. Indeed, the instantiations have to be recorded
in the context and are applied incrementally to each remaining judgment in
turn, rather than immediately to all remaining judgments at once. Also the
stack discipline of the ordered contexts does not mesh well with the use of
output contexts; explicit marker entries are needed in two places to
demarcate the end of an existential variable's scope. These are compelling 
reasons to look for a simpler algorithm that is easier to reason about in a 
mechanised setting.


%-------------------------------------------------------------------------------
\subsection{Small-Step Unification}
% Mention some algorithms for unification for 
% dependent types that use a small-step approach. Credit them later for some ideas that 
% we also employ.
There are literals that make use of small-step unification for dependently typed
inference and reconstruction algorithms~\cite{Reed2009,Abel2011higher}.
These approaches collect a list of constraints and process one at a time.
Similar to DK's algorithm, unification variables are used to represent the types to guess.
In order to solve unification variables to correctly scoped types
(typically constrained by the context when the variable is introduced),
the context information should be kept with the variables.
Judgments that represent unifications of terms might also require such a context
to keep the constraint collection well-formed.

As an example, Abel et al. defines their constraint $K$ by
\begin{gather*}
\begin{aligned}
K &::= {\top} \mid {\bot} &&\text{Trivial constraint and inconsistency.}\\
    & \ \mid \  \Psi\vdash M = N : C &&\text{Unify term $M$ with $N$.}\\
    & \ \mid \  \Psi\mid R:A \vdash E = E' &&\text{Unify evaluation context $E$ with $E'$.}\\
    & \ \mid \  \Psi\vdash u\leftarrow M: C &&\text{Solution for $u$ (metavariable) found.}
\end{aligned}
\end{gather*}
where the unification constraints and metavariable solutions are bound by a proper context.
A unification problem in their system is described by $\Delta \Vdash \overline{K}$,
where $\Delta$ is a collection of metavariables with their defined scopes.
This approach clearly states the scroping of each unification problem,
therefore rules out invalid instantiations to metavariables.

However, such ``duplicated contexts'' are not ideal for our formalization.
Following DK's algorithm, an existential variable $\al$ defined in the context
could be decomposed into a function type $\al[1] \to \al[2]$,
so the declaration of $\al$ should be replaced by two declarations $\al[1], \al[2]$.
Such operation causes all the context that contains $\al$ to change,
which brings difficulty on the synchronization of multiple contexts,
as variable declarations are not centralized.

We find a way of encoding multiple contexts by ``compressing'' them into a single worklist.
As our type inference rules applied to a judgment,
we typically keep the context or add variable declarations to the old context
before analysing its sub-judgment(s).
This enables us to write the judgment worklist in such a form:
$$\Gm, \{\text{variable declarations}\}, \{\text{judgment}\},
\{\text{variable declarations}\}, \{\text{judgment}\}, \ldots .$$
Typically, we simplify the right-most judgment and push back smaller tasks,
where the old ``context'' automatically gets inherited.
When some rules solve or partially solve an existential variable,
we could easily propagate the solution to all the judgments,
and safely modify its declaration, such as remove from the worklist.
New judgment(s) may also introduce new \emph{local} variables by declaring
that variable right before the new judgment(s).
After the new judgments are satisfied, these local variables are properly recycled.

% \jimmy{TODO explain the duplicated context with some examples (their judgment form)}
% Some issues to point out: Duplicated contexts (rather than shared contexts), which make 
% the formalization harder since requires ``synchronizing'' things ...

%-------------------------------------------------------------------------------
\subsection{Worklist Algorithm for Subtyping}

In recent work~\cite{itp2018}, we have shown how to mechanise a variant of DK's
subtyping algorithm and shown it correct with respect to DK's declarative
subtyping judgment. This approach overcomes the problem's in DK's formulation
by using a \emph{worklist}-based judgement of the form $$\Gamma \vdash \Omega$$
\jimmy{emphasize the form of judgment}
where $\Omega$ is a worklist, i.e. a sequence, of subtyping problems of the
form $A \leq B$.  The judgement is defined by case analysis on the first
element of $\Omega$ and recursively processes the worklist until it is empty.
Using both a single common ordered context $\Gamma$ and a worklist $\Omega$ greatly
simplifies propagating the instantiation of type variables that happen in one
subtyping problem to the remaining ones.

Unfortunately, this work does not solve all problems. In particular, it has two
major limitations that prevent it from scaling to the whole DK system. 

\paragraph{Scoping Garbage} Firstly, the single common type ordered context 
$\Gamma$ does not accurately reflect the type and unification variables
currently in scope. Instead, it is an overapproximation that steadily accrues
variables, and only drops those unification variables that are instantiated.
In other words, $\Gamma$ contains ``garbage'' that is no longer in scope.
This complicates establishing the relation with the declarative system.


\paragraph{No Inference Judgments} 
Secondly and more importantly, the approach only deals with a judgement for
\emph{checking} whether one type is the subtype of another. While this may not
so be difficult to scale to the \emph{checking} mode of term typing $\Gamma
\vdash e \Leftarrow A$, it clearly lacks the functionality to support the
\emph{inference} mode of term typing $\Gamma \vdash e \Rightarrow A$. Indeed,
the latter requires not only the communication of unification variable
instantiations from one typing problem to another, but also an inferred type.

%-------------------------------------------------------------------------------
\subsection{Algorithmic Type Inference for Higher-Ranked Types: Key Ideas}

Our new algorithmic type system addresses builds on the work above, but
addresses the open problems. 

\paragraph{Scope Tracking}
Firstly, we avoid scoping garbage by blending the ordered context and the
worklist into a single syntactic sort $\Gamma$, our algorithmic worklist. This
algorithmic worklist interleaves (type and term) variables with \emph{work}
like checking or inferring types of expressions. The interleaving keeps track
of the variable scopes in the usual, natural way: each variables is in scope of
anything in front of it in the worklist. If there is nothing in front, the
variable is no longer needed and can be popped from the worklist. This way, no
garbage builds up.

\jimmy{Compare with DK's algo: subsumption rule might cause leak}

\paragraph{Inference Judgements}
To express the DK inference judgements, we use a novel form of work entries in
the worklist: our algorithmic judgement chains $\omega$. In its simplest form,
such a judgement chain is just a check, like a subtyping check $A \leq B$ or a
term typecheck $e \Leftarrow A$.  The non-trivial forms of chains capture an
inference task together with the the work that depends on its outcome. For
instance, a type inference task takes the form $e \Rightarrow_a \omega$. It
denotes that we need to infer the type, say $A$, of expression $e$ and use it
in the chain $\omega$ by subsituting it for the placeholder type variable $a$.


% Our algorithm borrows some ideas from previous work, while adding new ones. 
% A small-step style processing worklists; \emph{Judgment Chains}; others. 

% Importantly we now deal with inference judgement.

